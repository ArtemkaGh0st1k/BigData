{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c07885a-2dc6-4c1c-affa-2f257851fb82",
   "metadata": {},
   "source": [
    "<br>\n",
    "    <center>МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ</center>\n",
    "    <center>федеральное государственное автономное образовательное учреждение высшего образования </center> <center>«Самарский национальный исследовательский университет имени академика С.П. Королева»</center>\n",
    "    <center>(Самарский университет)</center> </br>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<br>\n",
    "<center>Институт \t     информатики и кибернетики</center>                                                   \t  \n",
    "<center>Кафедра \t     технической кибернетики</center>                                                              \t\n",
    "</br>\n",
    "<br/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br>\n",
    "<center>ОТЧЕТ</center>\n",
    "<center>по лабораторной работе №3</center>\n",
    "\n",
    "<center>«Введение в Spark с использованием Python»</center>\n",
    "<br/>\n",
    "<center>по дисциплине <strong>«Большие данные»</strong></center>\n",
    "<br/>\n",
    "<center></center>\n",
    "</br>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<p style=\"text-align:right;\">Выполнил: Васильев А.С.\n",
    "<br>6131-010402D\n",
    "<br>    \n",
    "<br>Преподаватель: Попов С.Б.\n",
    "</p>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "    <br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<center>Самара 2025</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7869745aba19a842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T12:06:38.943562Z",
     "start_time": "2025-12-10T12:06:19.566902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Переменные окружения установлены\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:/hadoop\"\n",
    "os.environ[\"hadoop.home.dir\"] = \"C:/hadoop\"\n",
    "os.environ[\"PATH\"] = f\"C:/hadoop/bin;{os.environ.get('PATH', '')}\"\n",
    "\n",
    "print(\"Переменные окружения установлены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada6e6cb-33e8-432f-b254-9605e190a977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Версия PySpark: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(f\"Версия PySpark: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33fe9e42-e49c-47f7-a5aa-1a02cbb7a348",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[32m      4\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mKafkaRead\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.jars.packages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark:spark-sql-kafka-0-10_2.12:4.1.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.legacy.timeParserPolicy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLEGACY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark сессия создана успешно\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Артем\\Desktop\\Study\\1 sem\\BigData\\3 lab\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Артем\\Desktop\\Study\\1 sem\\BigData\\3 lab\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Артем\\Desktop\\Study\\1 sem\\BigData\\3 lab\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Артем\\Desktop\\Study\\1 sem\\BigData\\3 lab\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Артем\\Desktop\\Study\\1 sem\\BigData\\3 lab\\.venv\\Lib\\site-packages\\pyspark\\java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaRead\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:4.1.0\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark сессия создана успешно\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ceeddb-e126-430e-96cf-ea7e9c01dfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-NJHEMR1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>KafkaRead</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2577ce07100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4cbb86-2465-4767-8de4-e98fcf569916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a844314d-6472-4ceb-a0cc-15c3e8704e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_servers = \"localhost:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f43868-887f-4298-bce5-3d95fd7b0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic1 = 'rides1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e2f0ad1735cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+--------------------+-------------+-------+--------------------+\n",
      "| topic|partition|offset|           timestamp|timestampType|key_str|           value_str|\n",
      "+------+---------+------+--------------------+-------------+-------+--------------------+\n",
      "|rides1|        0|     0|2025-12-10 18:19:...|            0|      1|1,2020-07-01 00:2...|\n",
      "|rides1|        0|     1|2025-12-10 18:19:...|            0|      1|1,2020-07-01 00:0...|\n",
      "|rides1|        0|     2|2025-12-10 18:19:...|            0|      2|2,2020-07-01 00:1...|\n",
      "|rides1|        0|     3|2025-12-10 18:19:...|            0|      2|2,2020-07-01 00:3...|\n",
      "|rides1|        0|     4|2025-12-10 18:19:...|            0|      2|2,2020-07-01 00:3...|\n",
      "+------+---------+------+--------------------+-------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_servers = \"localhost:9092\"\n",
    "\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_servers) \\\n",
    "    .option(\"subscribe\", topic1) \\\n",
    "    .load()\n",
    "\n",
    "df = df.withColumn('key_str', df['key'].cast('string').alias('key_str')).drop(\n",
    "    'key').withColumn('value_str', df['value'].cast('string').alias('value_str')).drop('value')\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b5fec5-f97d-4846-b239-c83bc73eb592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topic='rides1', partition=0, offset=261, timestamp=datetime.datetime(2025, 12, 10, 18, 19, 5, 777000), timestampType=0, key_str='1', value_str='1,2020-07-01 01:48:38,2020-07-01 01:51:55,1,1.30,1,N,140,75,2,5.5,3,0.5,0,0,0.3,9.3,2.5'),\n",
       " Row(topic='rides1', partition=0, offset=262, timestamp=datetime.datetime(2025, 12, 10, 18, 19, 5, 777000), timestampType=0, key_str='1', value_str='1,2020-07-01 01:55:54,2020-07-01 02:05:23,1,3.20,1,N,236,230,2,11.5,3,0.5,0,0,0.3,15.3,2.5'),\n",
       " Row(topic='rides1', partition=0, offset=263, timestamp=datetime.datetime(2025, 12, 10, 18, 19, 5, 777000), timestampType=0, key_str='1', value_str='1,2020-07-01 01:48:27,2020-07-01 01:52:21,1,.30,1,N,158,249,1,4.5,3,0.5,1.65,0,0.3,9.95,2.5'),\n",
       " Row(topic='rides1', partition=0, offset=264, timestamp=datetime.datetime(2025, 12, 10, 18, 19, 5, 777000), timestampType=0, key_str='1', value_str='1,2020-07-01 01:49:27,2020-07-01 01:55:52,1,.80,1,N,140,237,2,6.5,3,0.5,0,0,0.3,10.3,2.5'),\n",
       " Row(topic='rides1', partition=0, offset=265, timestamp=datetime.datetime(2025, 12, 10, 18, 19, 5, 777000), timestampType=0, key_str='2', value_str='2,2020-07-01 01:49:40,2020-07-01 01:56:37,3,2.64,1,N,263,161,1,9,0.5,0.5,0,0,0.3,12.8,2.5')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dc1812b-d9b2-49b2-9c4d-2ac4ea315d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------------+------------+\n",
      "|vendor_id|passenger_count|trip_distance|total_amount|\n",
      "+---------+---------------+-------------+------------+\n",
      "|        1|              1|          1.5|         9.3|\n",
      "|        1|              1|          9.5|        27.8|\n",
      "|        2|              1|         5.85|        22.3|\n",
      "|        2|              1|          1.9|       14.16|\n",
      "|        2|              1|         1.25|         7.8|\n",
      "+---------+---------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "parsed_df = df.withColumn(\n",
    "    \"parsed\", split(col(\"value_str\"), \",\")\n",
    ").select(\n",
    "    col(\"parsed\")[0].cast(\"int\").alias(\"vendor_id\"),\n",
    "    col(\"parsed\")[3].cast(\"int\").alias(\"passenger_count\"),\n",
    "    col(\"parsed\")[4].cast(\"float\").alias(\"trip_distance\"),\n",
    "    col(\"parsed\")[16].cast(\"float\").alias(\"total_amount\"))\n",
    "\n",
    "parsed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9172c40-fc86-4916-9fd4-7df79084eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Результаты через Spark DataFrame ===\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "|vendor_id|trip_count|total_passengers|    total_distance|  total_amount_sum|\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "|        1|        87|             101|394.29999962449074|1976.6299829483032|\n",
      "|        2|       179|             242| 744.6499975193292|3709.1699678897858|\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, count\n",
    "\n",
    "result_spark = parsed_df.groupBy(\"vendor_id\").agg(\n",
    "    count(\"*\").alias(\"trip_count\"),\n",
    "    spark_sum(\"passenger_count\").alias(\"total_passengers\"),\n",
    "    spark_sum(\"trip_distance\").alias(\"total_distance\"),\n",
    "    spark_sum(\"total_amount\").alias(\"total_amount_sum\")\n",
    ").orderBy(\"vendor_id\")\n",
    "\n",
    "print(\"=== Результаты через Spark DataFrame ===\")\n",
    "result_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "570954b1-ca79-44b7-b959-a6cf90d1a1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Результаты через SQL ===\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "|vendor_id|trip_count|total_passengers|    total_distance|  total_amount_sum|\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "|        1|        87|             101|394.29999962449074|1976.6299829483032|\n",
      "|        2|       179|             242| 744.6499975193292|3709.1699678897858|\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_df.createOrReplaceTempView(\"rides\")\n",
    "\n",
    "result_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        vendor_id,\n",
    "        COUNT(*) as trip_count,\n",
    "        SUM(passenger_count) as total_passengers,\n",
    "        SUM(trip_distance) as total_distance,\n",
    "        SUM(total_amount) as total_amount_sum\n",
    "    FROM rides\n",
    "    GROUP BY vendor_id\n",
    "    ORDER BY vendor_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== Результаты через SQL ===\")\n",
    "result_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc6c5c31-4f3c-4cc0-a7e1-772a68ce9d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Результаты через pandas-on-Spark ===\n",
      "   vendor_id  trip_count  total_passengers  total_distance  total_amount_sum\n",
      "0          1          87               101      394.300000       1976.629983\n",
      "1          2         179               242      744.649998       3709.169968\n",
      "\n",
      "=== Результаты через pandas-on-Spark (способ 2) ===\n",
      "   vendor_id  trip_count  total_passengers  total_distance  total_amount_sum\n",
      "0          1          87               101      394.300000       1976.629983\n",
      "1          2         179               242      744.649998       3709.169968\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "ps_df = parsed_df.pandas_api()\n",
    "\n",
    "result_ps = ps_df.groupby(\"vendor_id\").agg(\n",
    "    trip_count=(\"vendor_id\", \"count\"), \n",
    "    total_passengers=(\"passenger_count\", \"sum\"),\n",
    "    total_distance=(\"trip_distance\", \"sum\"),\n",
    "    total_amount_sum=(\"total_amount\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "result_ps = ps_df.groupby(\"vendor_id\").agg(\n",
    "    trip_count=(\"vendor_id\", \"count\"),\n",
    "    total_passengers=(\"passenger_count\", \"sum\"),\n",
    "    total_distance=(\"trip_distance\", \"sum\"),\n",
    "    total_amount_sum=(\"total_amount\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "print(\"=== Результаты через pandas-on-Spark ===\")\n",
    "print(result_ps)\n",
    "\n",
    "result_ps2 = ps_df.groupby(\"vendor_id\").agg({\n",
    "    \"passenger_count\": \"sum\",\n",
    "    \"trip_distance\": \"sum\",\n",
    "    \"total_amount\": \"sum\"\n",
    "})\n",
    "\n",
    "trip_counts = ps_df.groupby(\"vendor_id\").size().rename(\"trip_count\")\n",
    "result_ps2 = result_ps2.join(trip_counts).reset_index()\n",
    "\n",
    "result_ps2.columns = [\"vendor_id\", \"total_passengers\", \"total_distance\", \"total_amount_sum\", \"trip_count\"]\n",
    "\n",
    "result_ps2 = result_ps2[[\"vendor_id\", \"trip_count\", \"total_passengers\", \"total_distance\", \"total_amount_sum\"]]\n",
    "\n",
    "print(\"\\n=== Результаты через pandas-on-Spark (способ 2) ===\")\n",
    "print(result_ps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c12499-d356-486a-9643-279fb9dd7790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Результаты через pandas ===\n",
      "   vendor_id  trip_count  total_passengers  total_distance  total_amount_sum\n",
      "0          1          87               101      394.299988       1976.630005\n",
      "1          2         179               242      744.650024       3709.169922\n"
     ]
    }
   ],
   "source": [
    "pandas_df = parsed_df.toPandas()\n",
    "\n",
    "result_pandas = pandas_df.groupby(\"vendor_id\").agg({\n",
    "    \"passenger_count\": \"sum\",\n",
    "    \"trip_distance\": \"sum\",\n",
    "    \"total_amount\": \"sum\"\n",
    "}).reset_index()\n",
    "\n",
    "trip_counts = pandas_df.groupby(\"vendor_id\").size().reset_index()\n",
    "trip_counts.columns = [\"vendor_id\", \"trip_count\"]  \n",
    "\n",
    "result_pandas = result_pandas.merge(trip_counts, on=\"vendor_id\")\n",
    "\n",
    "result_pandas = result_pandas.rename(columns={\n",
    "    \"passenger_count\": \"total_passengers\",\n",
    "    \"trip_distance\": \"total_distance\",\n",
    "    \"total_amount\": \"total_amount_sum\"\n",
    "})\n",
    "\n",
    "result_pandas = result_pandas[[\"vendor_id\", \"trip_count\", \"total_passengers\", \"total_distance\", \"total_amount_sum\"]]\n",
    "\n",
    "print(\"=== Результаты через pandas ===\")\n",
    "print(result_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6885ee8-e525-473a-808c-b08252aa7555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>9.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.50</td>\n",
       "      <td>27.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>22.299999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.90</td>\n",
       "      <td>14.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.25</td>\n",
       "      <td>7.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30</td>\n",
       "      <td>9.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.20</td>\n",
       "      <td>15.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>9.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.64</td>\n",
       "      <td>12.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     vendor_id  passenger_count  trip_distance  total_amount\n",
       "0            1                1           1.50      9.300000\n",
       "1            1                1           9.50     27.799999\n",
       "2            2                1           5.85     22.299999\n",
       "3            2                1           1.90     14.160000\n",
       "4            2                1           1.25      7.800000\n",
       "..         ...              ...            ...           ...\n",
       "261          1                1           1.30      9.300000\n",
       "262          1                1           3.20     15.300000\n",
       "263          1                1           0.30      9.950000\n",
       "264          1                1           0.80     10.300000\n",
       "265          2                3           2.64     12.800000\n",
       "\n",
       "[266 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8657441-d9f7-4e2f-a904-8fa2bb6323d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты сохранены в C:/tmp/results_by_vendor.csv\n",
      "   vendor_id  trip_count  total_passengers  total_distance  total_amount_sum\n",
      "0          1          87               101      394.300000       1976.629983\n",
      "1          2         179               242      744.649998       3709.169968\n"
     ]
    }
   ],
   "source": [
    "result_pd = result_spark.toPandas()\n",
    "result_pd.to_csv(\"C:/tmp/results_by_vendor.csv\", index=False)\n",
    "print(\"Результаты сохранены в C:/tmp/results_by_vendor.csv\")\n",
    "print(result_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f533ff4-324d-40d5-8418-b3d311462bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
