{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c07885a-2dc6-4c1c-affa-2f257851fb82",
   "metadata": {},
   "source": [
    "<br>\n",
    "    <center>МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ</center>\n",
    "    <center>федеральное государственное автономное образовательное учреждение высшего образования </center> <center>«Самарский национальный исследовательский университет имени академика С.П. Королева»</center>\n",
    "    <center>(Самарский университет)</center> </br>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<br>\n",
    "<center>Институт \t     информатики и кибернетики</center>                                                   \t  \n",
    "<center>Кафедра \t     технической кибернетики</center>                                                              \t\n",
    "</br>\n",
    "<br/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br>\n",
    "<center>ОТЧЕТ</center>\n",
    "<center>по лабораторной работе №3</center>\n",
    "\n",
    "<center>«Введение в Spark с использованием Python»</center>\n",
    "<br/>\n",
    "<center>по дисциплине <strong>«Большие данные»</strong></center>\n",
    "<br/>\n",
    "<center></center>\n",
    "</br>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<p style=\"text-align:right;\">Выполнил: Васильев А.С.\n",
    "<br>6131-010402D\n",
    "<br>    \n",
    "<br>Преподаватель: Попов С.Б.\n",
    "</p>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "    <br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<center>Самара 2025</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7869745aba19a842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T12:06:38.943562Z",
     "start_time": "2025-12-10T12:06:19.566902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Переменные окружения установлены\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:/hadoop\"\n",
    "os.environ[\"hadoop.home.dir\"] = \"C:/hadoop\"\n",
    "os.environ[\"PATH\"] = f\"C:/hadoop/bin;{os.environ.get('PATH', '')}\"\n",
    "\n",
    "print(\"Переменные окружения установлены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada6e6cb-33e8-432f-b254-9605e190a977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\pyspark_env\\lib\\socketserver.py\n",
      "['BaseRequestHandler', 'BaseServer', 'BufferedIOBase', 'DatagramRequestHandler', 'StreamRequestHandler', 'TCPServer', 'ThreadingMixIn', 'ThreadingTCPServer', 'ThreadingUDPServer', 'UDPServer', '_NoThreads', '_ServerSelector', '_SocketWriter', '_Threads', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '__version__', 'os', 'selectors', 'socket', 'sys', 'threading', 'time']\n",
      "Версия PySpark: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import socketserver\n",
    "print(socketserver.__file__)\n",
    "print(dir(socketserver))\n",
    "\n",
    "import pyspark\n",
    "print(f\"Версия PySpark: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33fe9e42-e49c-47f7-a5aa-1a02cbb7a348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark сессия создана успешно\n",
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaRead\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark сессия создана успешно\")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ceeddb-e126-430e-96cf-ea7e9c01dfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Gh0st1k:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>KafkaRead</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2ab9fd8b640>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4cbb86-2465-4767-8de4-e98fcf569916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a844314d-6472-4ceb-a0cc-15c3e8704e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_servers = \"localhost:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f43868-887f-4298-bce5-3d95fd7b0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic1 = 'rides1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e2f0ad1735cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+--------------------+-------------+-------+--------------------+\n",
      "| topic|partition|offset|           timestamp|timestampType|key_str|           value_str|\n",
      "+------+---------+------+--------------------+-------------+-------+--------------------+\n",
      "|rides1|        0|     0|2025-12-19 01:32:...|            0|      1|1,2020-07-01 00:2...|\n",
      "|rides1|        0|     1|2025-12-19 01:32:...|            0|      1|1,2020-07-01 00:0...|\n",
      "|rides1|        0|     2|2025-12-19 01:32:...|            0|      2|2,2020-07-01 00:1...|\n",
      "|rides1|        0|     3|2025-12-19 01:32:...|            0|      2|2,2020-07-01 00:3...|\n",
      "|rides1|        0|     4|2025-12-19 01:32:...|            0|      2|2,2020-07-01 00:3...|\n",
      "+------+---------+------+--------------------+-------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_servers = \"localhost:9092\"\n",
    "\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_servers) \\\n",
    "    .option(\"subscribe\", topic1) \\\n",
    "    .load()\n",
    "\n",
    "df = df.withColumn('key_str', df['key'].cast('string').alias('key_str')).drop(\n",
    "    'key').withColumn('value_str', df['value'].cast('string').alias('value_str')).drop('value')\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b5fec5-f97d-4846-b239-c83bc73eb592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topic='rides1', partition=0, offset=261, timestamp=datetime.datetime(2025, 12, 19, 1, 32, 6, 66000), timestampType=0, key_str='1', value_str='1,2020-07-01 01:48:38,2020-07-01 01:51:55,1,1.30,1,N,140,75,2,5.5,3,0.5,0,0,0.3,9.3,2.5'),\n",
       " Row(topic='rides1', partition=0, offset=262, timestamp=datetime.datetime(2025, 12, 19, 1, 32, 6, 66000), timestampType=0, key_str='1', value_str='1,2020-07-01 01:55:54,2020-07-01 02:05:23,1,3.20,1,N,236,230,2,11.5,3,0.5,0,0,0.3,15.3,2.5'),\n",
       " Row(topic='rides1', partition=0, offset=263, timestamp=datetime.datetime(2025, 12, 19, 1, 32, 6, 66000), timestampType=0, key_str='1', value_str='1,2020-07-01 01:48:27,2020-07-01 01:52:21,1,.30,1,N,158,249,1,4.5,3,0.5,1.65,0,0.3,9.95,2.5'),\n",
       " Row(topic='rides1', partition=0, offset=264, timestamp=datetime.datetime(2025, 12, 19, 1, 32, 6, 66000), timestampType=0, key_str='1', value_str='1,2020-07-01 01:49:27,2020-07-01 01:55:52,1,.80,1,N,140,237,2,6.5,3,0.5,0,0,0.3,10.3,2.5'),\n",
       " Row(topic='rides1', partition=0, offset=265, timestamp=datetime.datetime(2025, 12, 19, 1, 32, 6, 68000), timestampType=0, key_str='2', value_str='2,2020-07-01 01:49:40,2020-07-01 01:56:37,3,2.64,1,N,263,161,1,9,0.5,0.5,0,0,0.3,12.8,2.5')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dc1812b-d9b2-49b2-9c4d-2ac4ea315d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------------+------------+\n",
      "|vendor_id|passenger_count|trip_distance|total_amount|\n",
      "+---------+---------------+-------------+------------+\n",
      "|        1|              1|          1.5|         9.3|\n",
      "|        1|              1|          9.5|        27.8|\n",
      "|        2|              1|         5.85|        22.3|\n",
      "|        2|              1|          1.9|       14.16|\n",
      "|        2|              1|         1.25|         7.8|\n",
      "+---------+---------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "parsed_df = df.withColumn(\n",
    "    \"parsed\", split(col(\"value_str\"), \",\")\n",
    ").select(\n",
    "    col(\"parsed\")[0].cast(\"int\").alias(\"vendor_id\"),\n",
    "    col(\"parsed\")[3].cast(\"int\").alias(\"passenger_count\"),\n",
    "    col(\"parsed\")[4].cast(\"float\").alias(\"trip_distance\"),\n",
    "    col(\"parsed\")[16].cast(\"float\").alias(\"total_amount\"))\n",
    "\n",
    "parsed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9172c40-fc86-4916-9fd4-7df79084eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Результаты через Spark DataFrame ===\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "|vendor_id|trip_count|total_passengers|    total_distance|  total_amount_sum|\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "|        1|        87|             101|394.29999962449074|1976.6299829483032|\n",
      "|        2|       179|             242| 744.6499975193292|3709.1699678897858|\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, count\n",
    "\n",
    "result_spark = parsed_df.groupBy(\"vendor_id\").agg(\n",
    "    count(\"*\").alias(\"trip_count\"),\n",
    "    spark_sum(\"passenger_count\").alias(\"total_passengers\"),\n",
    "    spark_sum(\"trip_distance\").alias(\"total_distance\"),\n",
    "    spark_sum(\"total_amount\").alias(\"total_amount_sum\")\n",
    ").orderBy(\"vendor_id\")\n",
    "\n",
    "print(\"=== Результаты через Spark DataFrame ===\")\n",
    "result_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "570954b1-ca79-44b7-b959-a6cf90d1a1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Результаты через SQL ===\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "|vendor_id|trip_count|total_passengers|    total_distance|  total_amount_sum|\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "|        1|        87|             101|394.29999962449074|1976.6299829483032|\n",
      "|        2|       179|             242| 744.6499975193292|3709.1699678897858|\n",
      "+---------+----------+----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_df.createOrReplaceTempView(\"rides\")\n",
    "\n",
    "result_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        vendor_id,\n",
    "        COUNT(*) as trip_count,\n",
    "        SUM(passenger_count) as total_passengers,\n",
    "        SUM(trip_distance) as total_distance,\n",
    "        SUM(total_amount) as total_amount_sum\n",
    "    FROM rides\n",
    "    GROUP BY vendor_id\n",
    "    ORDER BY vendor_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== Результаты через SQL ===\")\n",
    "result_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc6c5c31-4f3c-4cc0-a7e1-772a68ce9d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Результаты через pandas-on-Spark ===\n",
      "   vendor_id  trip_count  total_passengers  total_distance  total_amount_sum\n",
      "0          1          87               101      394.300000       1976.629983\n",
      "1          2         179               242      744.649998       3709.169968\n",
      "\n",
      "=== Результаты через pandas-on-Spark (способ 2) ===\n",
      "   vendor_id  trip_count  total_passengers  total_distance  total_amount_sum\n",
      "0          1          87               101      394.300000       1976.629983\n",
      "1          2         179               242      744.649998       3709.169968\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "ps_df = parsed_df.pandas_api()\n",
    "\n",
    "result_ps = ps_df.groupby(\"vendor_id\").agg(\n",
    "    trip_count=(\"vendor_id\", \"count\"), \n",
    "    total_passengers=(\"passenger_count\", \"sum\"),\n",
    "    total_distance=(\"trip_distance\", \"sum\"),\n",
    "    total_amount_sum=(\"total_amount\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "result_ps = ps_df.groupby(\"vendor_id\").agg(\n",
    "    trip_count=(\"vendor_id\", \"count\"),\n",
    "    total_passengers=(\"passenger_count\", \"sum\"),\n",
    "    total_distance=(\"trip_distance\", \"sum\"),\n",
    "    total_amount_sum=(\"total_amount\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "print(\"=== Результаты через pandas-on-Spark ===\")\n",
    "print(result_ps)\n",
    "\n",
    "result_ps2 = ps_df.groupby(\"vendor_id\").agg({\n",
    "    \"passenger_count\": \"sum\",\n",
    "    \"trip_distance\": \"sum\",\n",
    "    \"total_amount\": \"sum\"\n",
    "})\n",
    "\n",
    "trip_counts = ps_df.groupby(\"vendor_id\").size().rename(\"trip_count\")\n",
    "result_ps2 = result_ps2.join(trip_counts).reset_index()\n",
    "\n",
    "result_ps2.columns = [\"vendor_id\", \"total_passengers\", \"total_distance\", \"total_amount_sum\", \"trip_count\"]\n",
    "\n",
    "result_ps2 = result_ps2[[\"vendor_id\", \"trip_count\", \"total_passengers\", \"total_distance\", \"total_amount_sum\"]]\n",
    "\n",
    "print(\"\\n=== Результаты через pandas-on-Spark (способ 2) ===\")\n",
    "print(result_ps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c12499-d356-486a-9643-279fb9dd7790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Результаты через pandas ===\n",
      "   vendor_id  trip_count  total_passengers  total_distance  total_amount_sum\n",
      "0          1          87               101      394.299988       1976.630005\n",
      "1          2         179               242      744.650024       3709.169922\n"
     ]
    }
   ],
   "source": [
    "pandas_df = parsed_df.toPandas()\n",
    "\n",
    "result_pandas = pandas_df.groupby(\"vendor_id\").agg({\n",
    "    \"passenger_count\": \"sum\",\n",
    "    \"trip_distance\": \"sum\",\n",
    "    \"total_amount\": \"sum\"\n",
    "}).reset_index()\n",
    "\n",
    "trip_counts = pandas_df.groupby(\"vendor_id\").size().reset_index()\n",
    "trip_counts.columns = [\"vendor_id\", \"trip_count\"]  \n",
    "\n",
    "result_pandas = result_pandas.merge(trip_counts, on=\"vendor_id\")\n",
    "\n",
    "result_pandas = result_pandas.rename(columns={\n",
    "    \"passenger_count\": \"total_passengers\",\n",
    "    \"trip_distance\": \"total_distance\",\n",
    "    \"total_amount\": \"total_amount_sum\"\n",
    "})\n",
    "\n",
    "result_pandas = result_pandas[[\"vendor_id\", \"trip_count\", \"total_passengers\", \"total_distance\", \"total_amount_sum\"]]\n",
    "\n",
    "print(\"=== Результаты через pandas ===\")\n",
    "print(result_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6885ee8-e525-473a-808c-b08252aa7555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>9.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.50</td>\n",
       "      <td>27.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>22.299999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.90</td>\n",
       "      <td>14.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.25</td>\n",
       "      <td>7.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30</td>\n",
       "      <td>9.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.20</td>\n",
       "      <td>15.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>9.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.64</td>\n",
       "      <td>12.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     vendor_id  passenger_count  trip_distance  total_amount\n",
       "0            1                1           1.50      9.300000\n",
       "1            1                1           9.50     27.799999\n",
       "2            2                1           5.85     22.299999\n",
       "3            2                1           1.90     14.160000\n",
       "4            2                1           1.25      7.800000\n",
       "..         ...              ...            ...           ...\n",
       "261          1                1           1.30      9.300000\n",
       "262          1                1           3.20     15.300000\n",
       "263          1                1           0.30      9.950000\n",
       "264          1                1           0.80     10.300000\n",
       "265          2                3           2.64     12.800000\n",
       "\n",
       "[266 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8657441-d9f7-4e2f-a904-8fa2bb6323d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты сохранены в C:/tmp/results_by_vendor.csv\n",
      "   vendor_id  trip_count  total_passengers  total_distance  total_amount_sum\n",
      "0          1          87               101      394.300000       1976.629983\n",
      "1          2         179               242      744.649998       3709.169968\n"
     ]
    }
   ],
   "source": [
    "result_pd = result_spark.toPandas()\n",
    "result_pd.to_csv(\"results_by_vendor.csv\", index=False)\n",
    "print(\"Результаты сохранены в C:/tmp/results_by_vendor.csv\")\n",
    "print(result_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f533ff4-324d-40d5-8418-b3d311462bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
