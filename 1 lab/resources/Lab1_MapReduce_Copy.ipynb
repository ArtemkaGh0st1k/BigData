{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br>\n",
                "    <center>МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ</center>\n",
                "    <center>федеральное государственное автономное образовательное учреждение высшего образования </center> <center>«Самарский национальный исследовательский университет имени академика С.П. Королева»</center>\n",
                "    <center>(Самарский университет)</center> </br>\n",
                "\n",
                "<br/>\n",
                "\n",
                "<br>\n",
                "<center>Институт \t     информатики и кибернетики</center>                                                   \t  \n",
                "<center>Кафедра \t     технической кибернетики</center>                                                              \t\n",
                "</br>\n",
                "<br/>\n",
                "\n",
                "<br>\n",
                "\n",
                "<br/>\n",
                "<br/>\n",
                "<br/>\n",
                "<br>\n",
                "<center>ОТЧЕТ</center>\n",
                "<center>по лабораторной работе №1</center>\n",
                "\n",
                "<center>«Введение в MapReduce модель на Python»</center>\n",
                "<br/>\n",
                "<center>по дисциплине <strong>«Большие данные»</strong></center>\n",
                "<br/>\n",
                "<center></center>\n",
                "</br>\n",
                "<br/>\n",
                "<br/>\n",
                "<br/>\n",
                "\n",
                "\n",
                "\n",
                "<br>\n",
                "<p style=\"text-align:right;\">Выполнил: Фамилия И.О.\n",
                "<br>613х-010402D\n",
                "<br>    \n",
                "<br>Преподаватель: Попов С.Б.\n",
                "</p>\n",
                "<br/>\n",
                "<br/>\n",
                "<br/>\n",
                "    <br/>\n",
                "<br/>\n",
                "<br/>\n",
                "<center>Самара 2025</center>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "82OvPKEiEqjc"
            },
            "source": [
                "# Введение в MapReduce модель на Python\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Vq3EWRIpwSiJ"
            },
            "source": [
                "# **Модель MapReduce**\n",
                "Функция MapReduce имитирует работу фреймворка MapReduce: подготовка и форматирование исходных данных в виде набора пар ключ-значение (key-value) для последующего вызова функции MAP, вызов функции MAP, группировка промежуточных результатов работы функции MAP по ключу и формирование массива значений для каждого ключа, вызов функции REDUCE.\n",
                "\n",
                "Пользователь для решения своей задачи реализует функции RECORDREADER, MAP, REDUCE."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "V1PZeQMwwVjc"
            },
            "outputs": [],
            "source": [
                "def flatten(nested_iterable):\n",
                "  for iterable in nested_iterable:\n",
                "    for element in iterable:\n",
                "      yield element\n",
                "\n",
                "def groupbykey(iterable):\n",
                "  t = {}\n",
                "  for (k2, v2) in iterable:\n",
                "    t[k2] = t.get(k2, []) + [v2]\n",
                "  return t.items()\n",
                "\n",
                "def MapReduce(RECORDREADER, MAP, REDUCE):\n",
                "  return flatten(map(lambda x: REDUCE(*x), groupbykey(flatten(map(lambda x: MAP(*x), RECORDREADER())))))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "iFIVrimep678"
            },
            "source": [
                "## Спецификация MapReduce\n",
                "\n",
                "\n",
                "\n",
                "```\n",
                "f (k1, v1) -> (k2,v2)*\n",
                "g (k2, v2*) -> (k3,v3)*\n",
                " \n",
                "mapreduce ((k1,v1)*) -> (k3,v3)*\n",
                "groupby ((k2,v2)*) -> (k2,v2*)*\n",
                "flatten (e2**) -> e2*\n",
                " \n",
                "mapreduce .map(f).flatten.groupby(k2).map(g).flatten\n",
                "```\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "R7az-6DA6qr2"
            },
            "source": [
                "## WordCount (пример)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "id": "dN-nbtgG6uYG",
                "outputId": "24117576-7931-401d-a581-28e246b23453"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[('it', 9), ('is', 9), ('what', 5), ('a', 1), ('banana', 1)]"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from typing import Iterator\n",
                "\n",
                "d1 = \"\"\"it is what it is\n",
                "it is what it is\n",
                "it is what it is\"\"\"\n",
                "d2 = \"\"\"what is it\n",
                "what is it\"\"\"\n",
                "d3 = \"\"\"it is a banana\"\"\"\n",
                "documents = [d1, d2, d3]\n",
                "\n",
                "def RECORDREADER():\n",
                "  for (docid, document) in enumerate(documents):\n",
                "    for (lineid, line) in enumerate(document.split('\\n')):\n",
                "      yield (\"{}:{}\".format(docid,lineid), line)\n",
                "\n",
                "def MAP(docId:str, line:str):\n",
                "  for word in line.split(\" \"):  \n",
                "    yield (word, 1)\n",
                " \n",
                "def REDUCE(word:str, counts:Iterator[int]):\n",
                "  sum = 0\n",
                "  for c in counts:\n",
                "    sum += c\n",
                "  yield (word, sum)\n",
                "\n",
                "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
                "output = list(output)\n",
                "output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "03IffTEOJgOb"
            },
            "source": [
                "## Вычисление TF-IDF (Term Frequency – Inverse Document Fraquency)\n",
                "\n",
                "Реализуется в три этапа:\n",
                "\n",
                "**Этап 1:** Частота слова в документе\n",
                "\n",
                "**Этап 2:** Количество документов, в которых встречается слово\n",
                "\n",
                "**Этап 3:** Расчёт TF-IDF\n",
                "\n",
                "Реализация включает:\n",
                "1. Формирование набора исходных данных: проиндексированные строки, содержащие текст аннотаций.\n",
                "2. Код функций RECORDREADER_1, MAP_1, REDUCE_1 для этапа 1.\n",
                "3. Код функций RECORDREADER_2, MAP_2, REDUCE_2 для этапа 2.\n",
                "4. Код функций RECORDREADER_3, MAP_3, REDUCE_3 для этапа 3.\n",
                "5. Последовательный вызов модельной функции Mapreduce для всех трёх этапов.\n",
                "6. Ввод на печать результата отдельно для каждого документа в виде первых пяти слов и их TF-IDF, упорядоченных по убыванию их значений (для каждого документа вывести на печать первые пять слов с максимальным TF-IDF)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Код для чтения данных из DOCX файла и реализации TF-IDF\n",
                "import zipfile\n",
                "import xml.etree.ElementTree as ET\n",
                "import re\n",
                "import math\n",
                "from collections import defaultdict\n",
                "from typing import Iterator\n",
                "from pathlib import Path\n",
                "\n",
                "def _docx_paragraphs(path):\n",
                "    # Открываем .docx как zip-архив и читаем XML основного документа\n",
                "    with zipfile.ZipFile(path) as z:\n",
                "        xml = z.read(\"word/document.xml\")\n",
                "    # Пространство имён WordprocessingML\n",
                "    ns = {\"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"}\n",
                "    # Парсим XML и собираем список параграфов\n",
                "    root = ET.fromstring(xml)\n",
                "    paras = []\n",
                "    for p in root.findall(\".//w:p\", ns):\n",
                "        # Внутри абзаца собираем все текстовые узлы <w:t>\n",
                "        texts = [t.text for t in p.findall(\".//w:t\", ns) if t.text]\n",
                "        # Склеиваем кусочки в одну строку абзаца (или пустую строку)\n",
                "        paras.append(\"\".join(texts) if texts else \"\")\n",
                "    # Нормализуем пробелы в каждом параграфе: сжимаем подряд и обрезаем края\n",
                "    return [re.sub(r\"\\s+\", \" \", t).strip() for t in paras]\n",
                "\n",
                "def parse_lab_data_from_docx(path):\n",
                "    # Получаем плоский список абзацев (уже без лишних пробелов)\n",
                "    paras = _docx_paragraphs(path)\n",
                "    \n",
                "    # -------- Извлечение документов (аннотаций) --------\n",
                "    docs, current = [], []\n",
                "    in_docs = False\n",
                "    \n",
                "    for line in paras:\n",
                "        # Заголовок раздела аннотации (начало нового документа)\n",
                "        if re.search(r\"^\\s*Аннотац\", line, re.IGNORECASE):\n",
                "            # если накапливался предыдущий документ - сохраняем\n",
                "            if current:\n",
                "                docs.append(\" \".join(current).strip())\n",
                "                current = []\n",
                "            in_docs = True\n",
                "            continue\n",
                "            \n",
                "        if in_docs:\n",
                "            # дошли до раздела со списком смежности - значит, аннотация закончилась\n",
                "            if re.search(r\"Список\\s+смежности\", line, re.IGNORECASE):\n",
                "                break\n",
                "            # копим непустые строки текущего документа\n",
                "            if line.strip():\n",
                "                current.append(line.strip())\n",
                "    \n",
                "        # финальный \"хвост\" документа, если остался\n",
                "    if current:\n",
                "        docs.append(\" \".join(current).strip())\n",
                "    \n",
                "    # -------- Извлечение списка смежности --------\n",
                "    adj = {}\n",
                "    in_adj = False\n",
                "    \n",
                "    for line in paras:\n",
                "        # Начало раздела \"Список смежности\"\n",
                "        if re.search(r\"Список\\s+смежности\", line, re.IGNORECASE):\n",
                "            in_adj = True\n",
                "            continue\n",
                "        \n",
                "        if in_adj and line.strip():\n",
                "            # Парсим строку вида \"0: [1, 3]\"\n",
                "            match = re.match(r\"(\\d+):\\s*\\[(.*?)\\]\", line.strip())\n",
                "            if match:\n",
                "                node_id = match.group(1)\n",
                "                neighbors_str = match.group(2)\n",
                "                # Извлекаем номера соседних узлов\n",
                "                neighbors = []\n",
                "                if neighbors_str.strip():\n",
                "                    neighbor_nums = re.findall(r\"\\d+\", neighbors_str)\n",
                "                    neighbors = [int(n) for n in neighbor_nums]\n",
                "                adj[node_id] = neighbors\n",
                "    \n",
                "    return docs, adj\n",
                "\n",
                "# Читаем данные из DOCX файла\n",
                "try:\n",
                "    docx_path = Path(\"tasks/Lab1_данные.docx\")\n",
                "    documents, adj = parse_lab_data_from_docx(docx_path)\n",
                "    print(f\"Успешно прочитано {len(documents)} документов из {docx_path}\")\n",
                "    for i, doc in enumerate(documents):\n",
                "        print(f\"Документ {i}: {doc[:100]}...\")\n",
                "except Exception as e:\n",
                "    print(f\"Ошибка чтения DOCX файла: {e}\")\n",
                "    # Fallback данные для тестирования\n",
                "    documents = [\n",
                "        \"The quick brown fox jumps over the lazy dog\",\n",
                "        \"A quick brown dog jumps over a lazy fox\",\n",
                "        \"The lazy brown dog is quick and jumps over the fox\"\n",
                "    ]\n",
                "    print(\"Используем тестовые данные\")\n",
                "\n",
                "# Настройки для TF-IDF\n",
                "N_DOCS = len(documents)\n",
                "print(f\"Общее количество документов: {N_DOCS}\")\n",
                "\n",
                "# Токенайзер: слова на латинице, допускаем дефис и апостроф внутри слова\n",
                "TOKEN_RE = re.compile(r\"[A-Za-z][A-Za-z\\-']+\")\n",
                "\n",
                "def tokenize(text: str):\n",
                "    # Находим все токены по шаблону и приводим к нижнему регистру\n",
                "    return [w.lower() for w in TOKEN_RE.findall(text)]\n",
                "\n",
                "# ====== ЭТАП 1: Частота слова в документе ======\n",
                "\n",
                "def RECORDREADER_1():\n",
                "    for doc_id, text in enumerate(documents):\n",
                "        yield (doc_id, text)\n",
                "\n",
                "def MAP_1(key, value: str):\n",
                "    doc_id = key\n",
                "    toks = tokenize(value)\n",
                "    for tok in toks:\n",
                "        yield ((doc_id, tok), 1)\n",
                "    # Подсчёт total токенов на документ\n",
                "    yield ((doc_id, \"*CNT*\"), len(toks))\n",
                "\n",
                "def REDUCE_1(key, values: Iterator[int]):\n",
                "    s = 0\n",
                "    for v in values:\n",
                "        s += v\n",
                "    yield (key, s)\n",
                "\n",
                "print(\"Выполняем этап 1 TF-IDF...\")\n",
                "stage1_out = list(MapReduce(RECORDREADER_1, MAP_1, REDUCE_1))\n",
                "print(f\"Этап 1 завершен. Получено {len(stage1_out)} записей\")\n",
                "\n",
                "# ====== ЭТАП 2: Количество документов, в которых встречается слово ======\n",
                "\n",
                "def RECORDREADER_2():\n",
                "    for kv in stage1_out:\n",
                "        yield kv\n",
                "\n",
                "def MAP_2(key, value):\n",
                "    # key == (doc_id, tok) OR (doc_id, \"*CNT*\")\n",
                "    doc_id, tok = key\n",
                "    if tok == \"*CNT*\":\n",
                "        yield ((\"TOTAL\", doc_id), value)\n",
                "    else:\n",
                "        # отмечаем факт присутствия терма в документе\n",
                "        yield ((tok, doc_id), 1)\n",
                "\n",
                "def REDUCE_2(key, values: Iterator[int]):\n",
                "    if key[0] == \"TOTAL\":\n",
                "        total = 0\n",
                "        for v in values:\n",
                "            total += v\n",
                "        yield (key, total)\n",
                "    else:\n",
                "        tok, doc_id = key\n",
                "        cnt = 0\n",
                "        for v in values:\n",
                "            cnt += v\n",
                "        if cnt > 0:\n",
                "            yield ((\"DF\", tok), doc_id)\n",
                "\n",
                "print(\"Выполняем этап 2 TF-IDF...\")\n",
                "stage2_out = list(MapReduce(RECORDREADER_2, MAP_2, REDUCE_2))\n",
                "print(f\"Этап 2 завершен. Получено {len(stage2_out)} записей\")\n",
                "\n",
                "# ====== ЭТАП 3: Расчёт TF-IDF ======\n",
                "\n",
                "def RECORDREADER_3():\n",
                "    for kv in stage1_out:\n",
                "        yield kv\n",
                "    for kv in stage2_out:\n",
                "        yield kv\n",
                "\n",
                "def MAP_3(key, value):\n",
                "    # stage1: ((doc, tok), cnt) | ((doc, \"*CNT*\"), total)\n",
                "    # stage2: ((\"TOTAL\", doc), total) | ((\"DF\", tok), doc_id)\n",
                "    if isinstance(key, tuple) and len(key) == 2 and isinstance(key[0], int):\n",
                "        doc_id, tok = key\n",
                "        if tok == \"*CNT*\":\n",
                "            yield ((\"TOTAL\", doc_id), value)\n",
                "        else:\n",
                "            yield ((\"TF\", doc_id, tok), (\"CNT\", value))\n",
                "    elif key[0] == \"TOTAL\":\n",
                "        _, doc_id = key\n",
                "        yield ((\"TOTAL\", doc_id), value)\n",
                "    elif key[0] == \"DF\":\n",
                "        _, tok = key\n",
                "        yield ((\"DFTOK\", tok), value)\n",
                "\n",
                "def REDUCE_3(group_key, values: Iterator):\n",
                "    if group_key[0] == \"TOTAL\":\n",
                "        total = 0\n",
                "        for v in values:\n",
                "            total += v\n",
                "        yield (group_key, total)\n",
                "    elif group_key[0] == \"DFTOK\":\n",
                "        seen = set(values)\n",
                "        df = len(seen)\n",
                "        yield ((\"DF\", group_key[1]), df)\n",
                "    elif group_key[0] == \"TF\":\n",
                "        cnt = 0\n",
                "        for tag, v in values:\n",
                "            if tag == \"CNT\":\n",
                "                cnt += v\n",
                "        yield (group_key, cnt)\n",
                "\n",
                "print(\"Выполняем этап 3 TF-IDF...\")\n",
                "stage3_intermediate = list(MapReduce(RECORDREADER_3, MAP_3, REDUCE_3))\n",
                "\n",
                "# Собираем TOTAL, DF, TF и считаем tf-idf\n",
                "totals, dfs, tfs_raw = {}, {}, {}\n",
                "for key, val in stage3_intermediate:\n",
                "    if key[0] == \"TOTAL\":\n",
                "        totals[key[1]] = val\n",
                "    elif key[0] == \"DF\":\n",
                "        dfs[key[1]] = val\n",
                "    elif key[0] == \"TF\":\n",
                "        _, doc_id, tok = key\n",
                "        tfs_raw[(doc_id, tok)] = val\n",
                "\n",
                "tfidf = defaultdict(dict)\n",
                "for (doc_id, tok), tn in tfs_raw.items():\n",
                "    cn = totals.get(doc_id, 1)\n",
                "    tf = tn / cn if cn else 0.0\n",
                "    df = dfs.get(tok, 1)\n",
                "    idf = math.log(N_DOCS / df) if df > 0 else 0.0\n",
                "    tfidf[doc_id][tok] = tf * idf\n",
                "\n",
                "# Выводим топ-5 слов для каждого документа\n",
                "print(\"\\nРезультаты TF-IDF - топ-5 слов для каждого документа:\")\n",
                "top5_per_doc = {doc_id: sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:5]\n",
                "                for doc_id, scores in tfidf.items()}\n",
                "\n",
                "for doc_id, pairs in sorted(top5_per_doc.items()):\n",
                "    print(f\"Документ {doc_id}: {[(w, round(s,6)) for w,s in pairs]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Поиск кратчайшего пути на графе с использованием алгоритма BFS"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Реализация включает:\n",
                "1. Формирование набора исходных данных: структура, содержащая списки смежности вершин исходного графа.\n",
                "2. Указание начальной и искомой вершин пути на графе.\n",
                "3. Код функции RECORDREADER, формирующей набор исходных данных для каждой итерации, в том числе и для первой итерации.\n",
                "4. Код функций  MAP и REDUCE для каждой итерации.\n",
                "5. Код функции, формирующей признак завершения итераций.\n",
                "6. Код цикла итераций алгоритма BFS, с вызовом модельной функции Mapreduce в теле цикла.\n",
                "7. Вывод на печать кратчайшего пути на графе от исходной до искомой вершины."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python [conda env:base] *",
            "language": "python",
            "name": "conda-base-py"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
